defaults:
  - agent: ourc

# mode
reward_free: true
# task settings
domain: walker # primal task will be infered in runtime
obs_type: states # [states, pixels]
frame_stack: 3 # only works if obs_type=pixels
action_repeat: 1 # set to 2 for pixels
discount: 0.99
# train settings
num_train_frames: 2000010
num_seed_frames: 100
# eval
eval_every_frames: 10000
num_eval_episodes: 10
# snapshot
snapshots: [100000, 500000, 1000000, 1500000, 2000000]
snapshot_dir: ../../models/${obs_type}/${domain}/${agent.name}/${seed}
# replay buffer
replay_buffer_size: 1000000
replay_buffer_num_workers: 4
batch_size: ${agent.batch_size}
nstep: ${agent.nstep}
update_encoder: true # should always be true for pre-training
# misc
seed: 3
device: cpu
save_video: true
save_train_video: false
use_tb: false
use_wandb: false
# experiment
experiment: ${agent.name}_seed_${seed}_${domain}
extrainfo: _
